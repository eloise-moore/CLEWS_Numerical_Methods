{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Methods Lab 4: More Data Analysis & Data Transformation \n",
    "\n",
    "[xkcd comic #26: Fourier](https://xkcd.com/26/)\n",
    "\n",
    "![xkcd: Fourier](fourier.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data \n",
    "Data filtering is the process of reducing data to the most relevant information - essentially \"clarifying\" the signal. There are a number of ways to filter data, for instance:\n",
    "\n",
    "* Sliding window / moving average \n",
    "* Gaussian moving average \n",
    "\n",
    "### Sliding window / moving average \n",
    "The sliding window works by creating a series of averages in \"windows\" of a dataset, where the window is essentially a fixed length over which the average is calculated. There are many ways to express it mathematically, one way is:\n",
    "\n",
    "$$ y_n = \\frac{1}{2k+1} \\sum^{k}_{i=-k} x_{n + i} $$\n",
    "\n",
    "Its purpose is to reduce the amplitude of noise while leaving the linear trend unchanged. There are a number of ways to implement this in Python, particularly depending on how you define boundary conditions for the average. \n",
    "\n",
    "Let's consider a simple example with no boundary conditions. We can define our noisy data as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "x = np.arange(0, 2 * np.pi, 0.01)\n",
    "y = np.sin(x) + np.random.uniform(size=len(x)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define a moving average for the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(da, k):\n",
    "    result = np.zeros(len(da))\n",
    "\n",
    "    for i in range(k+1, len(da) - (k+1)):\n",
    "        for j in range(-k, k+1):\n",
    "            result[i] += da[i+j]\n",
    "\n",
    "        result[i] = result[i] / (2*k + 1)\n",
    "\n",
    "    return result   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot the result of the moving average, with a window size of 41. Mathematically, it would look like this:\n",
    "\n",
    "$$ y_n = \\frac{1}{41} \\sum^{20}_{i=-20} x_{n + i} $$\n",
    "\n",
    "When plotting the moving average, we have trimmed the data so we don't see any zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x[21:-21], moving_average(y, 20)[21:-21])   # strip off the zeros \n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, scipy provides some functions to calculate the sliding window average. In the assignments, I prefer for you to create your own implementation of the function. \n",
    "\n",
    "To compute the sliding window average using scipy, we can use *uniform_filter1d* from scipy.ndimage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, uniform_filter1d(y, 41, mode='nearest'))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian moving average \n",
    "Another way to reduce noise in a dataset is to use the Gaussian moving average filter. It works exactly the same as the sliding window, except instead of using equal weights for each value in the calculation, it uses a Gaussian distribution to assign the weights. We can express it mathematically as \n",
    "\n",
    "$$ y_n = \\sum_{i=1}^k w_i(n)x_i$$ \n",
    "\n",
    "where \n",
    "\n",
    "$$ w_i(n) = \\frac{\\frac{1}{\\sqrt{2\\pi}}\\exp\\left[ \\frac{-(n-i)^2}{2k^2} \\right]}{\\sum_{i=1}^k\\frac{1}{\\sqrt{2\\pi}}\\exp\\left[ \\frac{-(n-i)^2}{2k^2} \\right]}$$\n",
    "\n",
    "One advantage of using the Gaussian weights in this moving average is that it is able to reduce noice far better than when using the sliding window average. One example implementation in Python is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_moving_average(da, k):\n",
    "    result = np.zeros(len(da))\n",
    "\n",
    "    for i in range(len(da)):\n",
    "        weights = (1 / (2 * np.pi)) * np.exp(-(np.arange(len(da)) - i)**2 / (2 * k**2))\n",
    "        weights = weights / sum(weights)\n",
    "        result[i] = sum(da * weights)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this on our mock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, gaussian_moving_average(y, 20))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that the Gaussian moving average reduces the noise in the data far more effectively than the sliding window average, and with a smaller window size!\n",
    "\n",
    "Of course, we do not need to manually implement this every time we would like to use the filter. We can use *gaussian_filter1d* from scipy.ndimage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter1d \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, gaussian_filter1d(y, 20, mode='nearest'))\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurements of correlation \n",
    "\n",
    "Correlation implies an interdependence of quantities, meaning that there is a statistical relationship between two variables, or two datasets. \n",
    "\n",
    "As an example, we can generate two time series, $a$ and $b$, which are created using values of each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros(50000)\n",
    "b = np.zeros(50000)\n",
    "\n",
    "for i in range(1, 50000) :\n",
    "    a[i] = a[i - 1] + 0.01 * b[i - 1]\n",
    "    b[i] = b[i - 1] - 0.05 * a[i - 1] - 0.01 * b[i - 1] + 0.001*np.random.randn() # add a little noise so that the arrays aren't just zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at this data together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax.plot(a, label=\"a\")\n",
    "ax.plot(b, label=\"b\")\n",
    "\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"a(t) or b(t)\")\n",
    "ax.set_title(\"a and b\")\n",
    "\n",
    "ax.set(xlim=[0, 1000])\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "ax2.plot(a, b, linewidth=0.5)\n",
    "ax2.set_xlabel(\"a\")\n",
    "ax2.set_ylabel(\"b\")\n",
    "ax2.set_title(\"a vs. b\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation\n",
    "\n",
    "Recall from the last lecture we were introduced to the Pearson correlation coefficient, that was essentially a measure of the strength of a linear relationship between two variables. It is defined as \n",
    "\n",
    "$$ \\rho_{x,y} = \\frac{\\text{cov}(x, y)}{\\sqrt{\\text{var}(x) \\text{var}(y)}} $$ \n",
    "\n",
    "One useful application of the Pearson correlation coefficient is within the *autocorrelation function*. The autocorrelation function measures the correlation between data in a time series $x_t$ with itself at a later time $x_{t + n}$ - this means that it is the Peason correlation between these values at later times.  There is a range of applications for the autocorrelation function in real data, for instance, you can use it to find a periodic signal that is hidden by noise. \n",
    "\n",
    "The autocorrelation function returns results on a scale from $-1$ to $1$. There are many trends that can exist in an autocorrelation plot, for instance:\n",
    "\n",
    "* **Positive autocorrelation**: when the autocorrelation values are positive, this suggests that there is a positive relationship between the past data points and the current data at a given lag. Depending on the amount of lag considered, this could indicate an upward trend in the data points, or even seasonality. \n",
    "\n",
    "* **Negative autocorrelation**: when the autocorrelation values are negative, this suggests that there is a negative relationship between the past data points and the current data at a given lag. Once again, depending on the amount of lag considered, this could indicate a downward trend in the data points, or seasonality. \n",
    "\n",
    "* **Autocorrelation around (or close to) zero**: this suggests that there is little to no relationship between the past data points and the current data. \n",
    "\n",
    "* **A peak in the autocorrelation**: at higher lags, there may be peaks in the autocorrelation - this suggests that there is a stronger relationship between the past data and the current data, and may represent a more-pronounced seasonality at that interval. \n",
    "\n",
    "* **Multiple peaks in the autocorrelation**: with a sufficient amount of lags considered, you may observe multiple peaks in the autocorrelation. This is a strong indication of seasonality in the data, and understanding the seasonal patterns present enables you to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson_correl(x,y):\n",
    "    return np.mean((x - np.mean(x)) * (y - np.mean(y))) / np.sqrt(np.var(x) * np.var(y))\n",
    "\n",
    "def autocorr(x,n):\n",
    "    \"\"\" \n",
    "    Parameters\n",
    "\n",
    "    x:  the series data to be tested for autocorrelation \n",
    "\n",
    "    n:  a series of numbers (lags) on which the autocorrelation function will be tested\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    T = len(n)\n",
    "    c = np.zeros(T)\n",
    "\n",
    "    for i in range(T):\n",
    "        c[i] = pearson_correl(x[0:N - n[i]] , x[n[i]:]) \n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute the autocorrelation function of our data, $a$ and $b$. Let's see how the first 1000 time steps are correlated to one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = np.arange(0,1000) \n",
    "\n",
    "c = autocorr(a, n)\n",
    "c2 = autocorr(b, n)\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax.plot(a)\n",
    "ax2.stem(c)\n",
    "\n",
    "ax.set(xlim=[0, 1000])\n",
    "\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"a(t)\")\n",
    "ax2.set_xlabel(\"n\")\n",
    "ax2.set_ylabel(\"autocorrelation\")\n",
    "ax.set_title(\"a\")\n",
    "ax2.set_title(\"autocorrelation of a\")\n",
    "plt.show()\n",
    "\n",
    "fig, (ax, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax.plot(b)\n",
    "ax2.stem(c2)\n",
    "\n",
    "ax.set(xlim=[0, 1000])\n",
    "\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"b(t)\")\n",
    "ax2.set_xlabel(\"n\")\n",
    "ax2.set_ylabel(\"autocorrelation\")\n",
    "ax.set_title(\"b\")\n",
    "ax2.set_title(\"autocorrelation of b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bars in this plot shows how much the past data impacts the following data. \n",
    "\n",
    "In an autocorrelation plot, the first value is always equal to $1$ - this is because the first data point can always explain the value of the first data point. \n",
    "\n",
    "What is really interesting in this plot is that we have a sort of sinusoidal pattern in the autocorrelation - this implies that there is some *seasonality* in the data! Further analysis can tell us more about the seasonality that we observe here. \n",
    "\n",
    "One way to do this is to compute the differences between the peaks of the autocorrelation function. Since the maximum is ways the first data point, one way to do this is to consider the difference between the peaks in the autocorrelation. A super neat way to do so (without calculus) is like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflection = np.diff(np.sign(np.diff(c))) # Find the second-order differences\n",
    "peaks = (inflection < 0).nonzero()[0] + 1 # Find where they are negative\n",
    "print(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This provides us the indices of the various peaks present in the autocorrelation function. Then, we can find the delay by working out which of these indices results in the maximum value in the autocorrelation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = peaks[c[peaks].argmax()]\n",
    "print(delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-correlation\n",
    "\n",
    "Cross-correlation is a measure of the correlation of a time series $x_t$ with another time series $y_{t+n}$ at a later time. It's useful to find the offset between two time series, and can be interpreted as:\n",
    "\n",
    "* **Positive cross-correlation**: positive values at a given lag indicates that the $x$ time series leads ahead of the $y$ time series\n",
    "* **Negative cross-correlation**: negative values at a given lag indicates that the $y$ time series leads ahead of the $x$ time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(x,y,n):\n",
    "    N = len(x)\n",
    "    T = len(n)\n",
    "    c = np.zeros(T)\n",
    "\n",
    "    for i in range(T):\n",
    "        c[i] = pearson_correl(x[0:N - n[i]], y[n[i]:])\n",
    "        \n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the cross-correlation of our $a$ and $b$ time series for the first 1000 timesteps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ab = crosscorr(a, b, n)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(c_ab)\n",
    "ax.set_xlabel(\"n\")\n",
    "ax.set_ylabel(\"cross-correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier transformations\n",
    "\n",
    "The idea behind the Fourier method is that any complicated function can be expressed as a sum of sine and cosine waves.\n",
    "\n",
    "Waves contain many characteristics, for instance:\n",
    "\n",
    "* Its *amplitude*, $A$, is a measure of the distance between the peak value and a baseline value\n",
    "\n",
    "* Its *wavelength*, $\\lambda$, measures the distance between successive peaks or troughs\n",
    "\n",
    "* Its *period*, $T$, measures the amount of time for a wave to make a complete cycle (e.g., from peak to peak, or trough to trough)  \n",
    "\n",
    "* Its *frequency*, $f$ measures how many waves pass through a fixed point in a given time frame. Typically, it can be measured as the number of cycles that pass within a time of 1 second, and has units of Hertz. The relationship between the period and frequency is \n",
    "\n",
    "$$ T = \\frac{1}{f} $$\n",
    "\n",
    "* Its *angular frequency*, $\\omega$, is a measure of the frequency but in radians per second. It is essentially a rotation rate, and is related to the frequency and period as \n",
    "\n",
    "$$ \\omega = 2\\pi f = \\frac{2\\pi}{T}$$\n",
    "\n",
    "* Its *phase*, $\\phi$, is an angle-like quantity that is a measure of the fraction of a cycle at a point in time. It can also be used as a measure of the offset between two waves in space or time. \n",
    "\n",
    "When measuring waves, we need to specify how often we sample the wave in time - this is known as *sampling*. Hence, the rate at which we sample the wave is the *sampling rate*. If we sample a wave at a rate of 20Hz, then that means that we are sampling 20 points in 1 second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier transform tells us the extent to which different frequencies are present in an original function or time series. Consider a function $f(x)$, then the Fourier transform is defined as \n",
    "\n",
    "$$ \\hat{f}(k) = \\int_{-\\infty}^\\infty f(x) \\exp\\left[ -i2\\pi k x\\right] \\: dx$$\n",
    "\n",
    "and its inverse is \n",
    "\n",
    "$$ f(x) = \\int_{-\\infty}^\\infty \\hat{f}(k) \\exp\\left[ i2\\pi k x\\right] \\: dk $$\n",
    "\n",
    "Recalling that Euler's formula\n",
    "\n",
    "$$ \\exp[ix] = \\cos(x) + i\\sin(x) $$\n",
    "\n",
    "is a way to link trigonometric functions to the complex domain.\n",
    "\n",
    "Note that these expressions represent the *continuous* Fourier transform, and this works when you have a continuous transformation of a continuous signal. \n",
    "\n",
    "In reality, we typically have a sample of a signal, i.e. the signal is *discrete*. This means that practically we use the discrete Fourier transform instead. The discrete Fourier transform transforms a sequence of evenly spaced signal to the information about the frequency of all the sine waves needed to sum up to the original time domain signal. This is defined as \n",
    "\n",
    "$$ X_k = \\sum_{x=0}^{N-1} x_n \\exp \\left[ \\frac{-i 2\\pi k n}{N}\\right] = \\sum_{x=0}^{N-1} x_n \\left[ \\cos\\left(\\frac{2\\pi kn}{N}\\right) - i\\sin\\left(\\frac{2\\pi kn}{N}\\right) \\right]$$\n",
    "\n",
    "where $N$ is the number of samples, $n$ is the current sample, $k$ is the current frequency where $k \\in [0, N-1]$, $x_n$ is the value of sine at a sample $n$, and $X_k$ is the discrete fourier transform containing information about the amplitude and phase. \n",
    "\n",
    "Let's consider a manual implementation of the discrete Fourier transform in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dft(x):\n",
    "    N = len(x) \n",
    "    n = np.arange(N)\n",
    "    k = n.reshape((N, 1))\n",
    "    \n",
    "    e = np.exp((-2j * np.pi * k * n) / N)\n",
    "\n",
    "    return np.dot(e, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few remarks on the implementation here: \n",
    "\n",
    "* The *reshape* function is a better way of converting a row vector into a column vector. It keeps the data points, but rearranges them into either a column vector, or a matrix as desired \n",
    "\n",
    "* Imaginary numbers in Python are represented by $j$ instead of $i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first consider a very simple example of a function made up of three sine waves of frequencies 1, 3 and 5, and amplitudes 3, 0.5 and 1. We will add up the sine waves with a sampling rate of 100Hz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 100 \n",
    "sampling_interval = 1 / sampling_rate \n",
    "\n",
    "t = np.arange(0, 1, sampling_interval)\n",
    "\n",
    "# first sine wave\n",
    "x = 3 * np.sin(2 * np.pi * 1 * t)\n",
    "\n",
    "# second sine wave \n",
    "x += 0.5 * np.sin(2 * np.pi * 3 * t)\n",
    "\n",
    "# third sine wave \n",
    "x += np.sin(2 * np.pi * 5 * t)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t, x)\n",
    "ax.set_xlabel(\"t\")\n",
    "ax.set_ylabel(\"amplitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's perform the discrete Fourier transform on this function obtain the frequencies of the sine waves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft_x = dft(x)\n",
    "\n",
    "# obtain the frequencies searched\n",
    "N = len(dft_x)\n",
    "n = np.arange(N)\n",
    "T = N / sampling_rate \n",
    "freq = n / T \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(freq, abs(dft_x), markerfmt=\" \")   # take the real part of the DFT\n",
    "ax.set_xlabel(\"frequency\")\n",
    "ax.set_ylabel(\"amplitude\")\n",
    "ax.set_title(\"Amplitude spectrum\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the output of the discrete Fourier transform is symmetric at a frequency of $\\sim 50$ - i.e. at half the sampling rate. This frequency is known as the *Nyquist frequency*, and is a result of the Nyquist-Shannon theorem stating that \"a signal sampled at a rate can be fully reconstructed provided it only contains frequency components that are below half of the sampling frequency\". This means that the highest frequency that can be output by a discrete Fourier transform is the Nyquist frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is symmetric, that means that we can simplify the plot to contain only one side of the transform result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_nyquist = int(sampling_rate / 2)\n",
    "\n",
    "freq_oneside = freq[:f_nyquist]\n",
    "dft_x_oneside = dft_x[:f_nyquist]\n",
    "\n",
    "# normalise the amplitude by the Nyquist frequency \n",
    "dft_x_oneside = dft_x_oneside / f_nyquist\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(freq_oneside, abs(dft_x_oneside), markerfmt=\" \")\n",
    "ax.set_xlabel(\"frequency\")\n",
    "ax.set_ylabel(\"amplitude\")\n",
    "ax.set_title(\"Amplitude spectrum\")\n",
    "# ax.set(xlim=[0,10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully resolved the frequencies and the amplitudes of the sine waves that were used to make up our function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, computing the discrete Fourier transform can be quite time-intensive, particularly if we use a large sampling rate. This is where the *Fast Fourier Transform* comes in! The Fast Fourier Transform is an algorithm that exploits some symmetries in the discrete Fourier transform in order to reduce complexity. \n",
    "\n",
    "Let's consider the equation for the discrete Fourier transform again:\n",
    "\n",
    "$$ X_k = \\sum_{x=0}^{N-1} x_n \\exp \\left[ \\frac{-i 2\\pi k n}{N}\\right]$$\n",
    "\n",
    "and let's compute the $(k+N)^\\text{th}$ term\n",
    "\n",
    "$$ \\begin{split}\n",
    "X_{k + N} & = \\sum_{x=0}^{N-1} x_n \\exp \\left[ \\frac{-i 2\\pi (k + N) n}{N}\\right] \\\\ \n",
    "& = \\sum_{x=0}^{N-1} x_n \\exp \\left[ \\frac{-i 2\\pi N n}{N}\\right] \\cdot \\exp \\left[ \\frac{-i 2\\pi k n}{N}\\right] \\\\\n",
    "&= \\sum_{x=0}^{N-1} x_n \\exp \\left[ -i 2\\pi n\\right] \\cdot \\exp \\left[ \\frac{-i 2\\pi k n}{N}\\right]\n",
    "\\end{split}$$\n",
    "\n",
    "However, $\\exp \\left[ -i 2\\pi n\\right] = 1$, so\n",
    "\n",
    "$$ X_{k + N} = \\sum_{x=0}^{N-1} x_n \\exp \\left[ \\frac{-i 2\\pi k n}{N}\\right] $$\n",
    "\n",
    "This implies that $X_{k + i\\cdot N} = X_k$ for any integer $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually need to manually implement the Fast Fourier Transform in Python, as it is provided by numpy. We can use it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fft\n",
    "\n",
    "fft_x = fft(x)\n",
    "fft_x_oneside = fft_x[:f_nyquist] / f_nyquist\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.stem(freq_oneside, abs(fft_x_oneside), markerfmt=\" \")\n",
    "ax.set_xlabel(\"frequency\")\n",
    "ax.set_ylabel(\"amplitude\")\n",
    "ax.set_title(\"Amplitude spectrum\")\n",
    "ax.set(xlim=[0, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy also calculates the sampling frequencies for you for a given window length $n$ and sampling interval $d$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.fft import fftfreq\n",
    "\n",
    "print(fftfreq(x.size, d=sampling_interval))\n",
    "print(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output frequencies from *fftfreq* are in a slightly different format: after the Nyquist frequency, the frequencies are mirror reflected instead."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
